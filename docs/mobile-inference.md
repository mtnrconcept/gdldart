# Int√©gration mobile hors-ligne de DeepDarts

Ce guide explique comment convertir le mod√®le DeepDarts au format ONNX puis l'int√©grer dans une application React Native pour effectuer l'inf√©rence directement sur l'appareil. Une alternative avec TensorFlow.js est √©galement fournie.

## 1. Exporter les poids TensorFlow vers ONNX

1. Installez les d√©pendances Python n√©cessaires¬†:
   ```bash
   pip install -r deep-darts-master/requirements.txt tf2onnx
   ```
2. Exportez le mod√®le vers ONNX avec le nouvel utilitaire¬†:
   ```bash
   cd deep-darts-master
   python export_to_onnx.py \
     --config deepdarts_d1 \
     --weights models/deepdarts_d1/weights \
     --output ../exports/deepdarts_d1.onnx
   ```
   Arguments utiles¬†:
   - `--config-path` pour charger un fichier YAML personnalis√©.
   - `--weights`/`--weights-type` pour pointer vers des poids sp√©cifiques (`.h5`, Darknet, etc.).
   - `--dynamic-batch` pour autoriser un batch dimension variable si vous comptez faire des batchs >1.

> üí° Si vous ne disposez que d'un fichier Keras `.h5`, l'outil le chargera automatiquement et g√©n√©rera `exports/deepdarts.onnx` par d√©faut.

## 2. (Optionnel) Conversion suppl√©mentaire

### Vers ONNX Runtime Mobile

1. Ajoutez `onnxruntime-tools` si vous souhaitez quantifier ou optimiser¬†:
   ```bash
   pip install onnxruntime-tools
   ```
2. Utilisez `onnxruntime_tools.optimizer` ou `onnxruntime.quantization` pour all√©ger le mod√®le (par exemple `float16` ou `dynamic quantization`).
3. Copiez le fichier ONNX dans le dossier assets de votre projet React Native (ex. `app/assets/models/deepdarts_d1.onnx`).

### Vers TensorFlow.js

1. Convertissez depuis le mod√®le Keras ou ONNX¬†:
   - Depuis Keras (`.h5`)¬†:
     ```bash
     tensorflowjs_converter \
       --input_format=keras \
       models/deepdarts_d1/weights.h5 \
       ../exports/tfjs-deepdarts-d1
     ```
   - Depuis ONNX via `onnx-tf` ou `tfjs-onnx` si vous avez besoin de couches sp√©cifiques.
2. Copiez les fichiers g√©n√©r√©s (`model.json` + shards) dans `app/assets/models/tfjs/`.

## 3. Configuration c√¥t√© React Native

### D√©pendances

Dans votre projet React Native, installez la cam√©ra et la biblioth√®que d'inf√©rence souhait√©e¬†:

- **ONNX Runtime (recommand√© pour les performances)**
  ```bash
  npm install onnxruntime-react-native react-native-vision-camera
  ```
- **TensorFlow.js**
  ```bash
  npm install @tensorflow/tfjs @tensorflow/tfjs-react-native react-native-vision-camera
  ```

Apr√®s l'installation, n'oubliez pas d'ex√©cuter `npx pod-install` sur iOS et de reconstruire l'application (Expo Go ne supporte pas ces modules natifs¬†; cr√©ez une dev build ou utilisez `expo prebuild`).

### Acc√©der aux frames cam√©ra

```ts
import { useFrameProcessor } from 'react-native-vision-camera';
import { runModel } from './model';

const frameProcessor = useFrameProcessor((frame) => {
  'worklet';
  const result = runModel(frame); // inf√©rence locale
  console.log('Dart detected:', result);
}, []);
```

- Utilisez `useCameraDevices()` de Vision Camera pour choisir la cam√©ra.
- R√©duisez la r√©solution envoy√©e au mod√®le (ex. 224√ó224) pour diminuer le co√ªt CPU/GPU.
- Ex√©cutez l'inf√©rence toutes les 300‚Äì500¬†ms plut√¥t qu'√† chaque frame.

### Chargement du mod√®le

#### ONNX Runtime

```ts
import { InferenceSession } from 'onnxruntime-react-native';

let session: InferenceSession | null = null;

export async function loadModel() {
  session = await InferenceSession.create('models/deepdarts_d1.onnx');
}

export async function runModel(frame: Frame) {
  if (!session) {
    await loadModel();
  }
  const tensor = frame.toTensor({ width: 224, height: 224 }); // pr√©process personnalis√©
  const feeds = { images: tensor };
  const results = await session.run(feeds);
  return postProcess(results);
}
```

#### TensorFlow.js

```ts
import '@tensorflow/tfjs-react-native';
import * as tf from '@tensorflow/tfjs';
import { bundleResourceIO } from '@tensorflow/tfjs-react-native';

let model: tf.GraphModel | null = null;

export async function loadModel() {
  await tf.ready();
  model = await tf.loadGraphModel(bundleResourceIO(modelJson, weightShards));
}

export async function runModel(frame: Frame) {
  if (!model) {
    await loadModel();
  }
  const tensor = preprocess(frame);
  const prediction = model.execute(tensor) as tf.Tensor;
  return decodeDetections(prediction);
}
```

Adaptez `preprocess`, `postProcess` et `decodeDetections` pour reproduire la logique de `predict.py` (`bboxes_to_xy`, `get_dart_scores`).

## 4. Optimisations mobiles

- Pr√©f√©rez un mod√®le l√©ger (YOLOv8n, MobileNet ou la version tiny de DeepDarts).
- R√©duisez la taille d'entr√©e (224√ó224 ou 320√ó320) et normalisez entre `[-1, 1]` ou `[0, 1]` selon l'entra√Ænement.
- Inf√©rence 1‚Äì3 fois par seconde suffit pour suivre les lancers.
- Envisagez la quantification (`float16`, `int8`) via ONNX Runtime ou TensorFlow Lite si n√©cessaire.

## 5. R√©sum√© du flux de travail

1. Exporter les poids TensorFlow ‚Üí ONNX (`export_to_onnx.py`).
2. (Optionnel) Optimiser/quantifier le mod√®le.
3. Ajouter le mod√®le √† votre projet React Native.
4. Installer `react-native-vision-camera` + biblioth√®que d'inf√©rence (ONNX ou TFJS).
5. Impl√©menter un frame processor qui appelle votre fonction d'inf√©rence locale.
6. Calculer le score √† partir des coordonn√©es `(x, y)` renvoy√©es.

Avec cette approche, toute la d√©tection se fait hors-ligne, sans latence r√©seau.
